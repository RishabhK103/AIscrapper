# ğŸ•·ï¸ AI Scrapper

AI Scrapper is a simple yet powerful web scraping and parsing tool built with **Streamlit**, **Selenium**, and **LangChain** powered by **Ollama (LLaMA 3)**. It lets you extract content from any webpage, clean it up, and then extract specific information using LLM-based natural language parsing.

---

## ğŸš€ Features

- âœ… Scrape content from any URL using a headless browser.
- ğŸ§¹ Clean and sanitize web content for easier processing.
- ğŸ“„ View and explore the scraped DOM content.
- ğŸ’¬ Ask the LLM to extract specific data from the scraped page using plain English.
- ğŸ”— Powered by `Ollama` and `LLaMA 3` through LangChain.

---

## ğŸ§± Project Structure

.
â”œâ”€â”€ main.py # Streamlit frontend interface
â”œâ”€â”€ parse.py # LLM-based parser using LangChain and Ollama
â”œâ”€â”€ scrape.py # Web scraping and content processing
â”œâ”€â”€ README.md # This file
â””â”€â”€ requirements.txt #python dependencies to run this project

## âš™ï¸ Requirements

- Python 3.11+
- Google Chrome + Chromedriver (path configured in `scrape.py`)
- [Ollama](https://ollama.com/) with LLaMA 3 installed locally
- Dependencies (see below)

---

## ğŸ§ª Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ai-scrapper.git
cd ai-scrapper
```
2. Create virtual environment (optional)
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

3. Install dependencies
```bash
pip install -r requirements.txt
```
4. Start Ollama and ensure llama3 is available
```bash
ollama run llama3
```
5. Run the app

```bash
streamlit run main.py
```

---

âœï¸ How to Use
Enter a website URL and click Scrape Site.

Review the cleaned DOM content if needed.

Type your natural language prompt describing what info you want to extract (e.g., "List all product names and prices").

Click Parse Content and let the LLM do the work.

ğŸ”’ Notes
This project assumes the LLM (Ollama) is running locally and accessible.

Avoid scraping websites that disallow bots (check robots.txt).

Not optimized for JavaScript-heavy or single-page applications (SPA) that need dynamic rendering.

ğŸ§  Example Use Cases
Extract product information from e-commerce sites.

Summarize article content from news websites.

Pull contact details or FAQs from business websites.


